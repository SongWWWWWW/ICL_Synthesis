{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"/home/test_yanjunchi/wangshaobo/ICL_Synthesis/generate/llama3.1_gsm8k_rand10p_normal_generate_4shot.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "for i in data:\n",
    "    print(\"----\")\n",
    "    # print(i[\"input\"])\n",
    "    print(i[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "def match_json_blocks(text): \n",
    "    json_block_pattern = r\"```json\\n(.*?)\\n```\" # 非贪婪匹配 \n",
    "    matches = re.findall(json_block_pattern, text, re.DOTALL) \n",
    "    return matches\n",
    "def match_curly_braces_json(text): \n",
    "    curly_braces_pattern = r\"\\{.*?###+\\{.*?\\}.*?\\}\" # 非贪婪匹配 \n",
    "    matches = re.findall(curly_braces_pattern, text, re.DOTALL) \n",
    "    # print(1)\n",
    "    return matches\n",
    "path = \"/home/test_yanjunchi/wangshaobo/ICL_Synthesis/generate/llama3.1_gsm8k_rand10p_normal_generate_4shot_prompt3.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "# data = data[:10]\n",
    "d = []\n",
    "a = 0\n",
    "for index,i in enumerate(data):\n",
    "    try:\n",
    "        m = match_curly_braces_json(i[\"output\"])\n",
    "    except Exception as e:\n",
    "        m = match_json_blocks(i[\"output\"])\n",
    "        try: \n",
    "            pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    try:\n",
    "        j = json.loads(m[0])\n",
    "        a+= 1\n",
    "        d.append(j)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "# print(a)\n",
    "# print(d)\n",
    "with open(\"llama3_prompt3_parse.json\",\"w\") as f:\n",
    "    json.dump(d,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "{\n",
    "  \"question\": \"Two friends, Emma and Olivia, are sharing some money in the ratio 3:2. If Emma received $480, how much money will Olivia have after giving $120 to her sister?\",\n",
    "  \"answer\": \"To find the total parts of the ratio, we add 3 + 2 = 5. Since Emma received 3 parts out of 5, 1 part is equal to $480 / 3 = $160. Olivia received 2 parts, so she received 2 x $160 = $320. After giving $120 to her sister, Olivia will have $320 - $120 = $200. Answer is ###{200}.\"\n",
    "}\n",
    "\"\"\"\n",
    "json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "# 示例字符串\n",
    "\n",
    "# 正则表达式匹配\n",
    "def parse(text):\n",
    "    pattern = re.compile(\n",
    "        r\"{'question': '(.*?)', 'answer': '(.*?)'}\"\n",
    "    )\n",
    "\n",
    "    # 匹配所有符合条件的字符串\n",
    "    matches = pattern.findall(text)\n",
    "    answers = []\n",
    "    # 输出结果\n",
    "    for match in matches:\n",
    "        question, answer = match\n",
    "        # print(\"Question:\", question)\n",
    "        # print(\"Answer:\", answer)\n",
    "        # print()\n",
    "        answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return answers\n",
    "path = \"/home/test_yanjunchi/wangshaobo/ICL_Synthesis/generate/llama3.1_gsm8k_rand10p_normal_generate_4shot.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "answer = []\n",
    "for index,i in enumerate(data):\n",
    "    ans = parse(i[\"output\"])\n",
    "    answer += ans\n",
    "print(len(answer))\n",
    "with open(\"llama3.1_gsm8k_rand10p_normal_generate_4shot_parse.json\",\"w\") as f:\n",
    "    json.dump(answer,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     30\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_message},\n\u001b[1;32m     32\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_message}\n\u001b[1;32m     33\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 调用 apply_chat_template 生成最终 prompt 字符串\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(formatted_prompt)\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_synthesis/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1615\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1617\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1620\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1622\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_synthesis/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1785\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1783\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1785\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1786\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1789\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         )\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 请将 model_name 修改为你的模型路径或名称\n",
    "model_name = \"/home/test_yanjunchi/wangshaobo/ICL_Synthesis/model/LLM-Research/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# 定义系统信息（system）和用户指令（user）\n",
    "system_message = \"You are an expert at generating math question-and-answer pairs.\"\n",
    "user_message = \"\"\"\n",
    "Task: Generate similar math question-and-answer pairs based on the provided reference sample. The generated Q&A should meet the following criteria:\n",
    "\n",
    "1. **Wide Range of Fields:** The questions should cover diverse mathematical fields, such as algebra, geometry, probability, statistics, calculus, and number theory.\n",
    "2. **Detailed and Rigorous Reasoning:** For each question, provide a clear, step-by-step solving process. Ensure that each step is mathematically rigorous and logically consistent.\n",
    "3. **Final Answer Format:** The final answer must be presented exactly in the format `###{{<result>}}`, where `<result>` is the correct numerical answer.\n",
    "4. **Strict JSON Format:** Output the results in JSON format, with each entry containing two keys: \"question\" and \"answer\". The \"answer\" field should include the entire reasoning process, concluding with the final answer in the specified format.\n",
    "\n",
    "**Reference Sample:**\n",
    "\n",
    "{data}\n",
    "\n",
    "Your task: Please generate at least 3 distinct Q&A pairs following the guidelines above. Each generated Q&A pair should:\n",
    "- Be diverse in terms of mathematical topics.\n",
    "- Provide a complete, step-by-step explanation for the solution.\n",
    "- End the reasoning with the final answer strictly formatted as ###{{<result>}}.\n",
    "\n",
    "Generate the output strictly in valid JSON format like the reference sample.\n",
    "\"\"\"\n",
    "\n",
    "# 构造聊天消息列表\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# 调用 apply_chat_template 生成最终 prompt 字符串\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(formatted_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_synthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
